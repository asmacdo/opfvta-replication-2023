\section{Discussion}

\subsection{Strengths of the original article as a reexecution target}
% this should be the shortest
The original paper made a significant effort to encapsulate all
components of the analysis as code, including the generation of images
and the rendering of the final pdf. Once properly installed, the
replication required little knowledge of the implementation details.

\subsection{Challenges}
Installation was challenging primarily because the process of installing
software on Gentoo is very slow. Building the container image took
about five hours, which lead to a slow development cycle where mistakes
were expensive.

\subsection{Lessons Learned}
% mention decimal rounding for localized text diff-ing

 - optimize the development cycle time.
   (bugfix -> rebuild container image -> push -> pull from compute -> run)
    If this takes a long time, very little work will get done
    - automate the build, push
    - Keep container images small
       - Keep data out of the images
       - use a small base image like alpine for your own containers
    - pin the gentoo repo as soon as dependencies are working, this will allow some use of the cache
    - copy as little into the image as possible. each unnecessary thing copied in may force a rebuild if it changes
  - modularize the workflow into isolated components where possible
    - separate containers when possible. Use a series of container "steps" rather than a "monolithic container" (analogy to microservices)
      In the enterprise world, the push to modernize has often started with "shoving what we have into containers", and this approach frequently is a headache and does not have the "cloud-native" benefits. Is it even worth doing this step?
      Instead, what is recommended for enterprise apps is to break the monolith into microservices that can all be deployed, scaled, and upgraded independently. 
      In science, it is obviously not as important to scale and upgrade without downtime, but the concept may still hold. 
      The way opfvta is organized makes it challenging to modularize because it was designed for reproducibility and that was accomplished by abstracting the internal workings from a single top-level "black box" interface.
      In this paper we have created the monolith, which suffers from slow builds, large sizes, and therefore a slow development cycle. 
    - What does a "chain" of BIDS apps look like?
    - easier and faster to debug and iterate
    - ideally, use images created by the projects with no modifications. ie, for fmriprep use fmriprep container
    - each container should have a well-defined "input" and "output" so it can be easily spotted which steps have failed.
  - separate jobs by participant where possible
    - instead of deploying 1 job to analyze all participants, deploy many jobs to each analyze 1 participant.
    - use scheduler like Slurm to manage resources rather than internal code
 - automate the orchestration of data
 - with singularity, automate the build from OCI push to datald repo, and then pull + datalad get from compute resource

\subsection{Outlook}
- The linking of the data might make re-use/hacking of the article for variant purposes more feasible
- Automation of reference results
- Separation of components for re-use
