\section{Results}
% What we accomplihesd, everything which we created, invented, etc. should preferentially go here.
% Descriptive, opinions either absent, or as objective-ish comments when facts are stated.
% Should, could, might, etc. sparingly and ideally not here but in discussion.


\subsection{Repository Structure}

In order to improve the reexecution reliability of the OPVFTA article we have constructed a parent repository which leverages Git and DataLad to link all reexecution requirements.
This framework leverages Git submodules for resource referencing, and DataLad \cite{datalad} in order to permit Git integration with data resources.

These resources include the original article, the raw data it operates on, and a reference mouse brain templates package, as submodules.
Additionally, it directly tracks the code required to coordinate the OPFVTA article reexecution and subsequent generation of \emph{this} article.
The code unique to the reexecution framework consists of container image generation and container execution instructions, and a Make system for process coordination (\cref{fig:topology}).

The Make system is structured into a top-level Makefile, which can be used for container image regeneration and upload, article reexecution in a containerized environment, and meta-article production.
Two article regeneration Make targets exist, for both the Open Container Initiative standard, and Singularity.
Both article reexecution targets wrap the \texttt{produce\_analysis.sh} script, which is a thin compatibility layer accessing the Make system of the original article.
The meta-article targets redirect to a Makefile in the \texttt{article/} child directory, which contains this document's human-readable text in \TeX format, alongside scripts for generating dynamical elements based on the reexecution results seen in the \texttt{outputs/} directory.

The layout constructed for this study thus provides robust provenance tracking and constitutes an instantiation of the YODA principle (a recursive acronym for “YODAs Organigram on Data Analysis”\cite{yoda}).

This repository structure diverges from the original reference article in directly linking the data at the repository level, as opposed to relying on its installation via a package manager.
%TODO chr Maybe add caveats to this in discussion
This leverages Git to provide basic dependency resolution, which adds portability across operating system choices as well as finer-grained version control.
Within the context of container usage, this also affords the advantage of keeping the data packages outside of container image generation logic, and not adding their disk space requirements to container images.

Notably, the article source code itself is not duplicated or further edited here, but handled as a Git submodule itself, with all proposed improvements being recorded in the original upstream repository.

\begin{figure*}
	\centering
	\includegraphics[clip,width=0.99\textwidth]{figs/topology.pdf}
	\caption{
		\textbf{The directory topology of the reexecution framework nests all requirements and includes a Make system for process coordination.}
		Depicted is a directory tree topology of the repository coordinating OPFVTA re-execution.
		Nested directories are represented by nested boxes, and Git submodules are highlighted with orange borders.
		The article reexecution PDF results are highlighted in light green, and the PDF of the resulting meta-article (i.e. this article) is highlighted in light blue.
	}
	\label{fig:topology}
\end{figure*}


%TODO: Something *descriptive* about how this new layout allows for more of a plug-in structure.


\subsection{Resource Refinement}

As a notable step in our article reproduction effort, we have updated resources previously only available as tarballs (i.e. compressed \texttt{tar} archives), to DataLad.
This refinement affords both the possibility to cherry-pick only required data files from the data archive (as opposed to requiring a full archive download), as well as more fine-grained version tracking capabilities.
In particular, our work encompassed the re-write of the Mouse Brain Templates package \cite{mbt05} Make system.
In its new release \cite{mbt10}, developed as part of this study, Mouse Brain Templates now publishes tarballs, as well as DataLad-accessible unarchived individual template files.


\subsection{Best Practice Guidelines}

As part of this work we have contributed substantial changes to the original OPFVTA repository, based on which we formulate a number of best practice guidelines, highly relevant in the production of reexecutable research outputs.

\subsubsection{Errors should be fatal more often than not}

By default programs written in the majority of languages (including e.g. Python and C) will just error out and exit immediately when running into an unexpected operation.
POSIX shell and other similar or derived ones, such as bash and zsh,  behave differently.
By default they continue with execution of the next scripted command, and only when the list of commands is exhausted or explicit `exit` command is called, it finally exits with a non-0 error code.
As a result, even if not causing some detrimental actions, an execution of the script could continue for hours before it fails, and original error message might be lost in the flood of output making it hard or impossible to localize original problem to be fixed.
This can be addressed by prepending \texttt{set -e} to the respective shell script, which changes the default behavior so that it will error out and stop execution as command exits with an error code.
Additionally, shell scripts treat undefined variables as a variable having an empty value, with empty values causing any error at all.
This can lead to numerous ill-defined behaviours, including a command such as \texttt{rm -rf "\${PREFIX}/"} removing all files on the system if \texttt{PREFIX} is not defined.
This can be addressed by prepending \texttt{set -u} so that an error is raised and execution is stopped as soon as undefined variable is attempted to be used.
To summarize, we advocate to include \texttt{set -eu} at the top of every shell script to guarantee it to error out as soon as any command fails or an undefined variable is attempted to be used.
Read more in the section on \href{https://www.repronim.org/module-reproducible-basics/01-shell-basics/\#fail-early}{``Command line/shell: Fail early''} subsection of the ``ReproNim Reproducible Basics Module''~\cite{repronim:reprobasics}.

\subsubsection{Avoid assuming or hard-coding absolute paths to resources}
Ensuring layout compatibility in different article execution environments is contingent on executable resources being able to find code or data required for execution.
Explicitly hardcoded in the scripts absolute paths are likely to not exist anywhere but the original execution environment, and hence scripts can very easily fail.
This problem is best avoided by adhering to YODA principles~\cite{yoda} of being able to reference all needed resources (data, scripts, container images, etc.) \emph{under} the study folder.
Use of relative paths within the study scripts then improve their portability.
Paths to external resources (scratch folders, reusable resources such as atlases etc. if not present within study folder) should be allowed to be specified dynamically via command line options or via an environment variable during execution.

\subsubsection{Avoid assuming a directory context for execution}
Per previous recommendation, resources may be linked via relative paths, these are paths resolved based on their hierarchical location with the execution base path as the reference point.
However, the scripts could be executed from various locations and not necessarily from the location of the script, thus making relative paths in scripts incorrect.
A good way of making a script utilizing relative paths more robust is ensuring that the script using relative path sets the base execution directory to its parent directory.
This can be accomplished in POSIX shell script via using \texttt{cd \textquotedbl\$(dirname \textquotedbl\$0\textquotedbl)\textquotedbl} at the beginning of the script.

\subsubsection{Workflow granularity greatly benefits efficiency}
The high time cost of executing the full analysis workflow makes debugging errors very time-consuming.
Ideally it should not be necessary to re-execute the entire workflow once an error is potentially resolved.
For this it is beneficial to formulate the workflow as many separate steps, where steps could be troubleshooted independently.
It is worth to separate out at least such large steps as preprocessing, individual levels of analysis (e.g. per-subject vs whole-population), and article generation.
YODA principles~\cite{yoda} again demonstrate one way how to organize digital artifacts across such stages for maximal re-usability.
An good feature is to use a workflow platform which automatically checks for the presence of results from prior stages, and, if present, proceed to the next stage without triggering re-computation of prior stages.
In this article we rely on simple GNU Make rules to provide very basic workflow across computational stages, but there exist hundreds of workflow engines and we refer reader to a recent call for FAIR workflows to discover more~\cite{fair-workflows}.

\subsubsection{Container image size should be kept small}
Due to a lack of persistency, addressing issues in container image contents requires rebuilding, which can be a time-consuming process.
The smaller the container image, the easier it is.
In particular, when using containers, it is thus advisable to \textit{not} provide data via a package manager or via manual download inside the build script.
A suitable solution is to assure provision of these resources outside of the container image and bind-mounting data thus downloaded to the host machine.

\subsubsection{Resources should be bundled into a DataLad superdataset}
As external resources might change or disappear, it is beneficial to use data version control system, such as git-annex and DataLad.
The git submodule mechanism allows to bundle multiple repositories together with clear versioning information, thus following the modularity principle promoted by YODA~\cite{yoda}.
Moreover, git-annex allows for multiple data sources and data integrity verification.

\subsubsection{Containers should fit the scope of the underlying workflow steps}
In order to not artificially extend the workload of rebuilding a container image, it is advisable to not create a bundled container image for sufficiently distinct separate steps of the workflow.
For an example, as seen in this study, the article reexecution container image should be distinct from container images required for producing a summary meta-article.
Complementary, and initially possibly appearing contradictory, to the aforementioned advise we recommend to avoid building separate containers for related steps, in particular if supported by the same toolkits, and rather define and use different \emph{entry points} to the same container.
E.g., a single container with AFNI could be used to access various tools from AFNI.
Similar approach is adopted by NeuroDesk~\cite{neurodesk} which provides a large collection of entry points for various tools from a smaller set of containers.

\subsubsection{Do not write debug-relevant data inside the container}
Debug-relevant data, such as intermediary data processing steps or debugging logs should not be deleted by the workflow, and further, should be written to persistent storage.
When using some containerization implementations, such as Docker, if file is written to a hard-coded path, as they would be on a persistent operating system, it will disappear once the container is removed.
Such file might be vital for debugging, and thus should not be lost.
This can be avoided by making sure that the paths used for intermediary and debugging outputs are bind-mounted to real directories on the parent system, from which they can be freely inspected.

\subsubsection{Parameterize scratch directories}
Complex workflows commonly generate large amounts of scratch data — intermediary data processing steps, with no other utility than being read by subsequent steps.
If these data are written to a hard-coded path, multiple executions will lead to race conditions, compromising one or multiple execution attempts.
This can be avoided by parameterizing the path and/or setting a default value based on a unique string (e.g. generated from the timestamps).
When using containers, this should be done at the container initiation level, as the relevant path is the path on the parent system, and not the path inside the container.

\subsubsection{Dependency versions inside container environments should be frozen as soon as feasible}
The need for full image rebuilding means that assuring consistent functionality in view of frequent updates is more difficult for containers than interactively managed environments.
This is compounded by the frequent and often API-breaking releases of many scientific software packages.
While dependency version freezing is not without cost in terms of assuring continued real-life functionality for an article, it can aid stable re-execution if this is done as soon as all required processing capabilities are provided.
How this is accomplished differs greatly based on the package manager used inside the container.
For example, PyPI, conda and other distributions can create an exact list of versioned packages present in the environment support package reinstall using that list.
Gentoo's Portage package manager allows to freeze versions by checking out a specific commit of the dependency tree, in view of which the package manager will automatically resolve the same versions.
Such an approach was used to ``reconstruct'' through freezing to prior point the computational environment used in the original paper.
% TODO: add href to point to the location in the repository where it freezes
On Debian, NeuroDebian-enhanced systems, and to a limited degree on Ubuntu systems, it is possible to use \texttt{nd\_freeze} utility from \texttt{neurodebian-freeze} package to make use of \href{https://snapshot.debian.org}{snapshot.debian.org} and \href{http://snapshot-neuro.debian.net}{snapshot-neuro.debian.net} and use frozen to specific timestamp state of the APT repositories, thus guaranteeing access to the same versions of packages.
NeuroDocker utility~\cite{neurodocker} provides further assistance in generating container recipes and has support for \texttt{nd\_freeze} to generate containers following the best practices.


\subsection{Reproduction Quality}

% TODO - make proper latex etc:
%  Additional aspects which were not foreseeing in the original execution making it impossible to reexecute to the identical result - no randomization seed was provided or recorded.

As a top-level view of reexecution results we have produced a simple infrastructure to analyze reproduction quality.
This provides both quality control for successful reexecution as well as a showcase of how automatic article reexecutability can be leveraged to evaluate \textit{reproducibility} at a glance.

For this purpose we compare the difference between the Historical Manuscript Record — a product of the original executable article generation — and multiple results generated via the New Reexecution Model.
Reproduction differences between the article versions are extracted by evaluating rasterized page-wise PDF difference (\ref{fig:diff_pages}).

\begin{figure*}
	\centering
	\includegraphics[clip,width=0.99\textwidth]{figs/diff_pages.pdf}
	\caption{
		\textbf{Page-wise visual differences between the Historical Manuscript Record and New Reexecution Model results help identify overall reproduction fidelity, and identify pages with noteworthy differences.}
		Depicted are rasterized document differences, weighted 1 for changes in any pixel color channel, and rounded to four decimal points.
		Error bars represent the \nth{95} percentile confidence interval.
	}
	\label{fig:diff_pages}
\end{figure*}

This overview shows a consistent minimum baseline of differing pixels between reexecutions, around $10^{-4}$ (i.e. \SI{0.01}{\percent}), best seen in pages 6 to 10.
When examined closely (\ref{fig:diff_date}), this difference corresponds to the modified date of the Historical Manuscript Record (2022-07-25) and the New Reexecution Model results (2023-..).
While otherwise inconsequential, this difference provides a good litmus test for whether the article was indeed reexecuted or simply preserved, and should be expected throughout all comparisons.
Throughout other pages we see difference percentages which are broadly consistent across reexecutions, but vary from page to page over almost 2 degrees of magnitude.
Upon inspection, more variable but comparatively lower-percentage differences (pages 4 and 5, detail depicted in \cref{fig:diff_text}) are revealed as text differences, arising from the original article generating dynamic inline statistic summaries.
Higher-percentage differences (detail depicted in \cref{fig:diff_fig}) correspond to dynamically generated data figures, in which high variability of nondeterministic preprocessing results in changes of the majority of figure pixels.

%TODO chr discuss this more in discussions.
Notably, inspecting these differences reveals a strong coherence at the qualitative evaluation level in spite of high quantitative variability.
This coherence manifests in the statements from the original article remaining valid with regard to statistical summaries which emerge from  \textit{de novo} data processing (as seen in \ref{fig:diff_text}, \ref{fig:diff_fig}).
This is particularly true for p-values, the magnitude of which can vary substantially at the lower tail of the distribution without impacting qualitative statements, as long as magnitude notation is used.

\begin{figure*}
	\centering
	\begin{subfigure}{0.99\textwidth}
		\centering
		\tcbox{
			\includegraphics[width=0.48\textwidth]{figs/diff_date.pdf}
			}
		\caption{
			The date change is correctly identified throughout the document, as seen in this example from page 1 of the article.
		}
		\label{fig:diff_date}
	\end{subfigure}
	\\
	\begin{subfigure}{0.99\textwidth}
		\centering
		\tcbox{
			\includegraphics[width=0.48\textwidth]{figs/diff_text.pdf}
			}
		\caption{
			Statistical summary values change, but maintain qualitative evaluation bracket with respect to e.g. p-value thresholds, as seen in this example from page 4 of the article.
		}
		\label{fig:diff_text}
	\end{subfigure}
	\\
	\vspace{1em}
	\begin{subfigure}{0.99\textwidth}
		\centering
		\tcbox{
			\includegraphics[width=0.8\textwidth]{figs/diff_fig.pdf}
			}
		\caption{
			In regression analysis, data points are highly variable, yet maintain a consistent slope and significance, as seen in this example from page 14 of the article.
		}
		\label{fig:diff_fig}
	\end{subfigure}
	\caption{
		\textbf{The article difference showcases expected quantitative and metadata variability, while maintaining overall validity of qualitative statements.}
		The figures are extracted from a full article \texttt{diff}, with tinted highlighting (blue for the Historical Manuscript Record, and orange for the New Reexecution Model result).
	}
	\label{fig:diff}
\end{figure*}

%TODO chr discuss this more in discussions.
Further, we find that text differences are well localized, as a function of the original article implementing fixed decimal rounding for statistical outputs (\cref{fig:diff}).
This, changes in the numerical value do not impact text length and do not generally propagate to subsequent lines, where they would be recorded as false positives.

\begin{figure*}
	\centering
	\begin{subfigure}{0.99\textwidth}
		\centering
		\tcbox{
			\includegraphics[width=0.48\textwidth]{figs/diff_date.pdf}
			}
		\caption{
			The date change is correctly identified throughout the document, as seen in this example from page 1 of the article.
		}
		\label{fig:diff_date}
	\end{subfigure}
	\\
	\begin{subfigure}{0.99\textwidth}
		\centering
		\tcbox{
			\includegraphics[width=0.48\textwidth]{figs/diff_text.pdf}
			}
		\caption{
			Statistical summary values change, but maintain qualitative evaluation bracket with respect to e.g. p-value thresholds, as seen in this example from page 4 of the article.
		}
		\label{fig:diff_text}
	\end{subfigure}
	\\
	\vspace{1em}
	\begin{subfigure}{0.99\textwidth}
		\centering
		\tcbox{
			\includegraphics[width=0.8\textwidth]{figs/diff_fig.pdf}
			}
		\caption{
			In regression analysis, data points are highly variable, yet maintain a consistent slope and significance, as seen in this example from page 14 of the article.
		}
		\label{fig:diff_fig}
	\end{subfigure}
	\caption{
		\textbf{The article difference showcases expected quantitative and metadata variability, while maintaining overall validity of qualitative statements.}
		The figures are extracted from a full article \texttt{diff}, with tinted highlighting (blue for the Historical Manuscript Record, and orange for the New Reexecution Model result).
	}
	\label{fig:diff}
\end{figure*}

